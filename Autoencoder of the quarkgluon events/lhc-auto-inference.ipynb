{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":305500,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":260649,"modelId":281804}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install gdown\n! gdown --id 1WO2K-SfU2dntGU4Bb3IYBp9Rh7rtTYEr -O filename\n! pip install h5p","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T12:07:51.194041Z","iopub.execute_input":"2025-03-28T12:07:51.194417Z","iopub.status.idle":"2025-03-28T12:08:04.918606Z","shell.execute_reply.started":"2025-03-28T12:07:51.194387Z","shell.execute_reply":"2025-03-28T12:08:04.917550Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.17.0)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.67.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2025.1.31)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1WO2K-SfU2dntGU4Bb3IYBp9Rh7rtTYEr\nFrom (redirected): https://drive.google.com/uc?id=1WO2K-SfU2dntGU4Bb3IYBp9Rh7rtTYEr&confirm=t&uuid=6d14ea3e-1226-439f-8853-43010e2a1f2d\nTo: /kaggle/working/filename\n100%|█████████████████████████████████████████| 701M/701M [00:04<00:00, 174MB/s]\n\u001b[31mERROR: Could not find a version that satisfies the requirement h5p (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for h5p\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nimport h5py\nfrom tqdm import tqdm\nimport os\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.tensorboard import SummaryWriter\nimport matplotlib.pyplot as plt\nimport concurrent.futures\nimport gc\n\n# ========================\n# Configuration\n# ========================\nconfig = {\n    'batch_size': 64,\n    'num_epochs': 80,\n    'initial_lr': 1e-3,\n    'patience': 5,\n    'min_lr': 1e-6,\n    'num_workers': min(8, os.cpu_count()),  # Limit to 8 workers max\n    'pin_memory': True,\n    'persistent_workers': True,\n    'sparse_threshold': 1e-6,\n    'save_full_precision': True,\n    'precision_decimals': 8\n}\n\n\n\n# Set numerical precision\ntorch.set_printoptions(precision=config['precision_decimals'])\nnp.set_printoptions(precision=config['precision_decimals'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T12:08:04.920134Z","iopub.execute_input":"2025-03-28T12:08:04.920484Z","iopub.status.idle":"2025-03-28T12:08:23.142275Z","shell.execute_reply.started":"2025-03-28T12:08:04.920435Z","shell.execute_reply":"2025-03-28T12:08:23.141448Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class JetAutoencoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 64, 3, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(256, 512, 3, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2),\n            nn.Flatten(),\n            nn.Linear(512*8*8, 4096),\n            nn.ReLU()\n        )\n        \n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(4096, 512*8*8),\n            nn.ReLU(),\n            nn.Unflatten(1, (512, 8, 8)),\n            nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1, output_padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 3, 3, stride=2, padding=1, output_padding=1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        return self.decoder(self.encoder(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T12:08:23.143903Z","iopub.execute_input":"2025-03-28T12:08:23.144503Z","iopub.status.idle":"2025-03-28T12:08:23.151967Z","shell.execute_reply.started":"2025-03-28T12:08:23.144454Z","shell.execute_reply":"2025-03-28T12:08:23.150949Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# 3. Your custom loss function\nclass WeightedMSE(nn.Module):\n    def __init__(self, weight_nonzero=1000.0):\n        super().__init__()\n        self.weight_nonzero = weight_nonzero  # Higher weight for non-zero pixels\n\n    def forward(self, y_pred, y_true):\n        # Create weight tensor: 1.0 for zeros, `weight_nonzero` for non-zeros\n        weights = torch.where(y_true == 0, \n                            torch.tensor(1.0, device=y_true.device),\n                            torch.tensor(self.weight_nonzero, device=y_true.device))\n        \n        # Calculate weighted MSE\n        squared_error = (y_true - y_pred) ** 2\n        weighted_loss = weights * squared_error\n        return torch.mean(weighted_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T12:08:23.153482Z","iopub.execute_input":"2025-03-28T12:08:23.153789Z","iopub.status.idle":"2025-03-28T12:08:23.191206Z","shell.execute_reply.started":"2025-03-28T12:08:23.153758Z","shell.execute_reply":"2025-03-28T12:08:23.190361Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class HandleSparseImages:\n    def __call__(self, img):\n        sparse_mask = (img < config['sparse_threshold'])\n        noise = torch.randn_like(img) * config['sparse_threshold'] * 0.1\n        return torch.where(sparse_mask, img + noise, img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T12:08:23.191986Z","iopub.execute_input":"2025-03-28T12:08:23.192258Z","iopub.status.idle":"2025-03-28T12:08:23.211618Z","shell.execute_reply.started":"2025-03-28T12:08:23.192235Z","shell.execute_reply":"2025-03-28T12:08:23.210892Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ========================\n# Data Loading\n# ========================\ndef load_data(i):\n    print(\"Loading data with full precision...\")\n    \n    end = i*25000\n    first = end - 25000\n    if end > 125000:\n        end = 139306\n        first = 125000\n\n    # if i>1:\n    #     del X_jets\n    #     gc.collect()  # Force garbage collector to run immediately\n\n    with h5py.File('/kaggle/working/filename', 'r') as f:\n        X_jets = f['X_jets'][first:end].astype(np.float32)\n    \n    if config['save_full_precision']:\n        #np.save('X_jets_original.npy', X_jets)\n        print(\"Saved original data with full precision\")\n    \n    transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: (x - x.min()) / (x.max() - x.min())),\n        HandleSparseImages()\n    ])\n    \n    def process_batch(batch):\n        return torch.stack([transform(img) for img in batch])\n    \n    print(\"Processing images with all workers...\")\n    batches = np.array_split(X_jets, config['num_workers'])\n    results = []\n    \n    with concurrent.futures.ThreadPoolExecutor(max_workers=config['num_workers']) as executor:\n        futures = [executor.submit(process_batch, batch) for batch in batches]\n        for future in tqdm(concurrent.futures.as_completed(futures), \n                         total=len(futures),\n                         desc=\"Processing\"):\n            results.append(future.result())\n    \n    return torch.cat(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T12:08:23.212593Z","iopub.execute_input":"2025-03-28T12:08:23.212901Z","iopub.status.idle":"2025-03-28T12:08:23.231411Z","shell.execute_reply.started":"2025-03-28T12:08:23.212872Z","shell.execute_reply":"2025-03-28T12:08:23.230535Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Initialize\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Initialize model\nmodel = JetAutoencoder().to(device)\n\n# 4. Load your trained model\nmodel_path = '/kaggle/input/lhc-auto-encoder/pytorch/default/1/best_model.pth'\nstate_dict  = torch.load(model_path, map_location=device)\nmodel.load_state_dict(state_dict)\n\ncriterion = WeightedMSE()\noptimizer = optim.Adam(model.parameters(), lr=config['initial_lr'])\nscheduler = ReduceLROnPlateau(optimizer, 'min', \n                            patience=config['patience'],\n                            min_lr=config['min_lr'],\n                            verbose=True)\nwriter = SummaryWriter()\n\n# Training\nbest_loss = float('inf')\nearly_stop_counter = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T12:08:23.232364Z","iopub.execute_input":"2025-03-28T12:08:23.232674Z","iopub.status.idle":"2025-03-28T12:08:33.296827Z","shell.execute_reply.started":"2025-03-28T12:08:23.232647Z","shell.execute_reply":"2025-03-28T12:08:33.295873Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-7-286ca17fc6cb>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict  = torch.load(model_path, map_location=device)\n/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"epoch = 0\nfor i in range(1,6):\n\n    # Load data\n    test_data = load_data(i)\n    \n    val_loader = DataLoader(test_data, batch_size=config['batch_size'],\n                               num_workers=config['num_workers'],\n                               pin_memory=config['pin_memory'])\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    val_pbar = tqdm(val_loader,\n                   desc=f'Epoch {epoch+1}/{config[\"num_epochs\"]} [Val]')\n    \n    with torch.no_grad():\n        for batch in val_pbar:\n            batch = batch.to(device, non_blocking=True)\n            outputs = model(batch)\n            loss = criterion(outputs, batch)\n            val_loss += loss.item()\n            val_pbar.set_postfix({\n                'val_loss': format(loss.item(), f\".{config['precision_decimals']}f\")\n            })\n    \n    val_loss /= len(val_loader)\n    scheduler.step(val_loss)\n    \n    writer.add_scalar('Loss/val', val_loss, epoch)\n    writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n    \n    print(f\"\\nEpoch {epoch+1} Summary:\")\n    print(f\"Val Loss: {val_loss:.{config['precision_decimals']}f}\")\n\n    del test_data\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T12:08:33.298440Z","iopub.execute_input":"2025-03-28T12:08:33.298731Z","iopub.status.idle":"2025-03-28T12:13:22.546258Z","shell.execute_reply.started":"2025-03-28T12:08:33.298707Z","shell.execute_reply":"2025-03-28T12:13:22.545488Z"}},"outputs":[{"name":"stdout","text":"Loading data with full precision...\nSaved original data with full precision\nProcessing images with all workers...\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|██████████| 4/4 [00:27<00:00,  6.87s/it]\nEpoch 1/80 [Val]: 100%|██████████| 391/391 [00:12<00:00, 31.14it/s, val_loss=0.01738904]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 Summary:\nVal Loss: 0.01556302\nLoading data with full precision...\nSaved original data with full precision\nProcessing images with all workers...\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|██████████| 4/4 [00:24<00:00,  6.01s/it]\nEpoch 1/80 [Val]: 100%|██████████| 391/391 [00:12<00:00, 32.19it/s, val_loss=0.02780784]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 Summary:\nVal Loss: 0.02678649\nLoading data with full precision...\nSaved original data with full precision\nProcessing images with all workers...\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|██████████| 4/4 [00:24<00:00,  6.20s/it]\nEpoch 1/80 [Val]: 100%|██████████| 391/391 [00:12<00:00, 30.94it/s, val_loss=0.02384840]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 Summary:\nVal Loss: 0.02975228\nLoading data with full precision...\nSaved original data with full precision\nProcessing images with all workers...\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|██████████| 4/4 [00:23<00:00,  5.94s/it]\nEpoch 1/80 [Val]: 100%|██████████| 391/391 [00:12<00:00, 31.84it/s, val_loss=0.03107654]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 Summary:\nVal Loss: 0.03027023\nLoading data with full precision...\nSaved original data with full precision\nProcessing images with all workers...\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|██████████| 4/4 [00:22<00:00,  5.75s/it]\nEpoch 1/80 [Val]: 100%|██████████| 391/391 [00:12<00:00, 31.54it/s, val_loss=0.02475519]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 Summary:\nVal Loss: 0.03112698\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}